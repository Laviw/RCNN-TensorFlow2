{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "crnn_tf.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "HWPIYt8kkh7l",
        "KjCAvEkkAJsq"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CijtXKvD70uA"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from tensorflow.keras.layers import Lambda, Dense, Bidirectional, GRU, Flatten, TimeDistributed, Permute, Activation, Input\n",
        "from tensorflow.keras.layers import LSTM, Reshape, Conv2D, MaxPooling2D, BatchNormalization, ZeroPadding2D\n",
        "from tensorflow.keras import backend as K\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "vqgLGqjlf-8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_dict = np.loadtxt('/content/dictionary.txt', dtype=np.str)\n",
        "print(label_dict)\n",
        "print(len(label_dict))\n",
        "num_classes = len(label_dict) + 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MtBcUe0gbLI",
        "outputId": "5d46aad8-d6ff-4a7c-abaf-53ce65ee5d04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['blank' '，' '的' ... '柒' '¥' '：']\n",
            "5996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "KjCAvEkkAJsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_extractor(input):\n",
        "    initializer = keras.initializers.he_normal()\n",
        "    x = Conv2D(64, (3,3), strides=(1,1), padding=\"same\", \n",
        "               kernel_initializer=initializer, \n",
        "               use_bias=True, name='conv2d_1')(input) # 32*128*64 \n",
        "    x = BatchNormalization(name=\"BN_1\")(x)\n",
        "    x = Activation(\"relu\", name=\"relu_1\")(x)\n",
        "    x = MaxPooling2D(pool_size=(2,2), strides=2, padding='valid', name='maxpl_1')(x) # 16*64*64\n",
        "\n",
        "    x = Conv2D(128, (3,3), strides=(1,1), padding=\"same\", \n",
        "               kernel_initializer=initializer, use_bias=True, \n",
        "               name='conv2d_2')(x) # 16*64*128\n",
        "    x = BatchNormalization(name=\"BN_2\")(x)\n",
        "    x = Activation(\"relu\", name=\"relu_2\")(x)\n",
        "    x = MaxPooling2D(pool_size=(2,2), strides=2, padding='valid', name='maxpl_2')(x) # 8*32*128\n",
        "    \n",
        "    x = Conv2D(256, (3,3), strides=(1,1), padding=\"same\", \n",
        "               kernel_initializer=initializer, use_bias=True, \n",
        "               name='conv2d_3')(x)  # 8*32*256\n",
        "    x = BatchNormalization(name=\"BN_3\")(x)\n",
        "    x = Activation(\"relu\", name=\"relu_3\")(x)\n",
        "\n",
        "    x = Conv2D(256, (3,3), strides=(1,1), padding=\"same\", \n",
        "               kernel_initializer=initializer, use_bias=True, \n",
        "               name='conv2d_4')(x) # 8*32*256\n",
        "    x = BatchNormalization(name=\"BN_4\")(x)\n",
        "    x = Activation(\"relu\", name=\"relu_4\")(x)\n",
        "    x = MaxPooling2D(pool_size=(2,1), strides=(2,1), name='maxpl_3')(x) # 4*32*256\n",
        "    \n",
        "    x = Conv2D(512, (3,3), strides=(1,1), padding=\"same\", \n",
        "               kernel_initializer=initializer, use_bias=True, \n",
        "               name='conv2d_5')(x) # 4*32*512\n",
        "    x = BatchNormalization(axis=-1, name='BN_5')(x)\n",
        "    x = Activation(\"relu\", name='relu_5')(x)\n",
        "\n",
        "    x = Conv2D(512, (3,3), strides=(1,1), padding=\"same\", \n",
        "               kernel_initializer=initializer, use_bias=True, \n",
        "               name='conv2d_6')(x) # 4*32*512\n",
        "    x = BatchNormalization(axis=-1, name='BN_6')(x)\n",
        "    x = Activation(\"relu\", name='relu_6')(x)\n",
        "    x = MaxPooling2D(pool_size=(2,1), strides=(2,1), name='maxpl_4')(x) # 2*32*512\n",
        "    \n",
        "    x = Conv2D(512, (2,2), strides=(1,1), padding='same', \n",
        "               activation='relu', kernel_initializer=initializer, \n",
        "               use_bias=True, name='conv2d_7')(x) # 2*32*512\n",
        "    x = BatchNormalization(name=\"BN_7\")(x)\n",
        "    x = Activation(\"relu\", name=\"relu_7\")(x)\n",
        "    conv_otput = MaxPooling2D(pool_size=(2, 1), name=\"conv_output\")(x) # 1*32*512\n",
        "    \n",
        "    return conv_otput"
      ],
      "metadata": {
        "id": "F_fl8p4O8FBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rnn(input):\n",
        "    initializer = keras.initializers.he_normal()\n",
        "\n",
        "    x = Permute((2, 3, 1), name='permute')(input) # 32*512*1\n",
        "    rnn_input = TimeDistributed(Flatten(), name='for_flatten_by_time')(x) # 32*512\n",
        "\n",
        "    # RNN part\n",
        "    y = Bidirectional(LSTM(256, kernel_initializer=initializer, return_sequences=True), \n",
        "                      merge_mode='sum', name='LSTM_1')(rnn_input) # 32*512\n",
        "    y = BatchNormalization(name='BN_8')(y)\n",
        "\n",
        "    rnn_output = Bidirectional(LSTM(256, kernel_initializer=initializer, \n",
        "                                return_sequences=True), name='LSTM_2')(y) \n",
        "\n",
        "    return rnn_output"
      ],
      "metadata": {
        "id": "elVi1CkM8G5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(input_shape=(32, 280, 1), num_classes=6004, max_label_len=23, is_training=True):\n",
        "    cnn_inputs = Input(shape=input_shape, name='image_input')\n",
        "    cnn_output = feature_extractor(cnn_inputs)\n",
        "    rnn_output = rnn(cnn_output)\n",
        "    y_pred = Dense(num_classes, activation='softmax', name='y_pred')(rnn_output)\n",
        "    base_model = keras.models.Model(inputs=cnn_inputs, outputs=y_pred)\n",
        "    return base_model"
      ],
      "metadata": {
        "id": "tV2PGWdu8IRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(num_classes=num_classes)\n",
        "# model.summary()"
      ],
      "metadata": {
        "id": "YgrZIyQX-TrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://github.com/Liumihan/CRNN_kreas/raw/master/trained_weights/300wbest_vgg_blstm_ctc_best_weight.h5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6xjxmSg4Tjg",
        "outputId": "0f3a7759-4ab6-4a24-ef29-352363991f31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-24 03:30:24--  https://github.com/Liumihan/CRNN_kreas/raw/master/trained_weights/300wbest_vgg_blstm_ctc_best_weight.h5\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Liumihan/CRNN_kreas/master/trained_weights/300wbest_vgg_blstm_ctc_best_weight.h5 [following]\n",
            "--2021-12-24 03:30:24--  https://raw.githubusercontent.com/Liumihan/CRNN_kreas/master/trained_weights/300wbest_vgg_blstm_ctc_best_weight.h5\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 45118132 (43M) [application/octet-stream]\n",
            "Saving to: ‘300wbest_vgg_blstm_ctc_best_weight.h5’\n",
            "\n",
            "300wbest_vgg_blstm_ 100%[===================>]  43.03M   281MB/s    in 0.2s    \n",
            "\n",
            "2021-12-24 03:30:26 (281 MB/s) - ‘300wbest_vgg_blstm_ctc_best_weight.h5’ saved [45118132/45118132]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "org_model = build_model(num_classes=5991)\n",
        "org_model.load_weights('/content/300wbest_vgg_blstm_ctc_best_weight.h5')"
      ],
      "metadata": {
        "id": "guPP4gTQUkQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(32):\n",
        "    model.layers[i].set_weights(org_model.layers[i].get_weights())"
      ],
      "metadata": {
        "id": "lxhjfxxFUyLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "Rb3qgEhTAMHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip contract_data.zip"
      ],
      "metadata": {
        "id": "yO5VyfJJAE4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "def load_images(image_path, image_size):\n",
        "    image = cv2.imdecode(np.fromfile(image_path, dtype=np.uint8),\n",
        "                         cv2.IMREAD_COLOR)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    image = cv2.resize(image, (image_size[1], image_size[0]))\n",
        "    image = image.astype(np.float32)\n",
        "    return image\n"
      ],
      "metadata": {
        "id": "43G1QBL4_zto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import Sequence\n",
        "class BatchGenerator(Sequence):\n",
        "    \"\"\"Generator for the input data to the OCR model. We're also preparing\n",
        "    arrays for the CTC loss which are related to the output dimensions\"\"\"\n",
        "\n",
        "    def __init__(self, contents, batch_size,\n",
        "                 img_size, down_sample_size, test=True, start=0):\n",
        "        super(BatchGenerator, self).__init__()\n",
        "        self.contents = contents[start:]\n",
        "        self.batch_size = batch_size\n",
        "        self.img_h, self.img_w = img_size\n",
        "        self.test = test\n",
        "        self.single_pred_len = int(self.img_w // down_sample_size)\n",
        "\n",
        "        # total number of unique characters\n",
        "        self.num_chars = 6004\n",
        "        self.epoch_size = int(len(contents)//batch_size)\n",
        "        self.image_amount = len(contents)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Denotes the number of batches per epoch\n",
        "        :return: number of batches per epoch \"\"\"\n",
        "        return self.epoch_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Generate one batch of data\"\"\"\n",
        "\n",
        "        # stores the length (number of characters) of each word in a batch\n",
        "        label_length = np.zeros((self.batch_size, 1), dtype=np.float64)\n",
        "        pred_length = np.full((self.batch_size, 1), self.single_pred_len, dtype=np.float64)\n",
        "\n",
        "        data_contents = self.contents[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        data_contents = [data.replace('\\n', '') for data in data_contents]\n",
        "\n",
        "        batch_images = []\n",
        "        batch_labels = []\n",
        "\n",
        "        max_word_len_batch = max([len(word.split(' ')[1:]) for word in data_contents])\n",
        "        for idx, content in enumerate(data_contents):\n",
        "            image_path = '/content/contract_data/' + content.split(' ')[0]\n",
        "            batch_images.append(load_images(image_path, image_size=(self.img_h, self.img_w)))\n",
        "\n",
        "            label_str = content.split(' ')[1:]\n",
        "            label_length[idx][0] = len(label_str)\n",
        "            label = [int(l_str) for l_str in label_str]\n",
        "            for n in range(max_word_len_batch - len(label_str)):\n",
        "                label.append(num_classes)\n",
        "\n",
        "            if not self.test:\n",
        "                label.append(len(label_str))\n",
        "                label.append(self.single_pred_len)\n",
        "            batch_labels.append(label)\n",
        "        batch_images = np.array(batch_images, dtype=np.float64) / 255. * 2 - 1\n",
        "        batch_images = np.expand_dims(batch_images, axis=-1)\n",
        "        batch_labels = np.array(batch_labels, dtype=np.float64)\n",
        "\n",
        "        return batch_images, batch_labels"
      ],
      "metadata": {
        "id": "CJIAF21q_qvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/correct_labels.txt', 'r', encoding='utf-8') as file:\n",
        "    dataset = file.readlines()\n",
        "    train_dataset = dataset[:int(len(dataset) * 0.9)]\n",
        "    validation_dataset = dataset[int(len(dataset) * 0.9):]\n",
        "\n",
        "print(len(train_dataset))\n",
        "print(len(validation_dataset))\n",
        "train_data = BatchGenerator(contents=train_dataset, batch_size=32, \n",
        "                img_size=(32, 280), down_sample_size=4,\n",
        "                validation=False)\n",
        "\n",
        "validation_data = BatchGenerator(contents=train_dataset, batch_size=32, \n",
        "                img_size=(32, 280), down_sample_size=4,\n",
        "                validation=False)"
      ],
      "metadata": {
        "id": "izBnDxybompQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e97f2c13-e278-441d-e32a-131f28aae5c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1846\n",
            "206\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# loss"
      ],
      "metadata": {
        "id": "NCa9NNSyXwFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sparse_labels(y_true):\n",
        "    labels = y_true[:, :-2]\n",
        "    label_length = y_true[:, -2]\n",
        "    logit_length = y_true[:, -1]\n",
        "    labels = tf.cast(labels, dtype=tf.int32)\n",
        "    label_length = tf.cast(label_length, dtype=tf.int32)\n",
        "    label_length = tf.expand_dims(label_length, axis=-1)\n",
        "    logit_length = tf.cast(logit_length, dtype=tf.int32)\n",
        "    logit_length = tf.expand_dims(logit_length, axis=-1)\n",
        "\n",
        "    return labels, label_length, logit_length\n",
        "\n",
        "# for i in range(1):\n",
        "#     x, y = train_data.__getitem__(i)\n",
        "#     y = tf.convert_to_tensor(y)\n",
        "#     labels, label_len, logit_len = sparse_labels(y)\n",
        "#     print(labels[:2])\n",
        "#     print(label_len[:2])\n",
        "#     print(logit_len[:2])"
      ],
      "metadata": {
        "id": "9gXob4OUADx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyCTCLoss(tf.keras.losses.Loss):\n",
        "    def call(self, y_true, y_pred):\n",
        "        labels, label_length, logit_length = sparse_labels(y_true)\n",
        "\n",
        "        # loss_value = tf.nn.ctc_loss(labels=labels,\n",
        "        #                logits=y_pred,\n",
        "        #                label_length=label_length,\n",
        "        #                logit_length=logit_length,\n",
        "        #                logits_time_major=False)\n",
        "        loss_value = tf.keras.backend.ctc_batch_cost(y_true=labels,\n",
        "                                y_pred=y_pred,\n",
        "                                input_length=logit_length,\n",
        "                                label_length=label_length)\n",
        "        \n",
        "        return tf.reduce_mean(loss_value)\n"
      ],
      "metadata": {
        "id": "RyNK9G5_F_Et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics"
      ],
      "metadata": {
        "id": "pb2BJjY5mgbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(y_true, y_pred):\n",
        "    labels, label_length, logit_length = sparse_labels(y_true)\n",
        "    \n",
        "    batch_size = labels.shape[0]\n",
        "    logit_length = tf.squeeze(logit_length)\n",
        "\n",
        "    if len(logit_length.shape) == 0:\n",
        "        logit_length = [tf.keras.backend.get_value(logit_length)]\n",
        "    else:\n",
        "        logit_length = tf.keras.backend.get_value(logit_length)\n",
        "    y_pred_list, _ = keras.backend.ctc_decode(y_pred, logit_length, greedy=True)\n",
        "    pred_label_tensor = y_pred_list[0]\n",
        "    pred_label = np.array(tf.keras.backend.get_value(pred_label_tensor))\n",
        "    pred_label = pred_label[:, :labels.shape[1]]\n",
        "    # for i in range(len(pred_label)):\n",
        "    pred_label[np.where(pred_label == -1)] = num_classes\n",
        "    m = tf.keras.metrics.Accuracy()\n",
        "\n",
        "    m.update_state(pred_label, labels)\n",
        "    return m.result()\n",
        "\n"
      ],
      "metadata": {
        "id": "mIB6KtOZrCJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compile and Train"
      ],
      "metadata": {
        "id": "nFKyHbBVX1_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss = MyCTCLoss()\n",
        "model.compile(optimizer=optimizer, loss=loss, run_eagerly=True, metrics=[accuracy])"
      ],
      "metadata": {
        "id": "pYA6XzHaKjke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_data,\n",
        "    steps_per_epoch=train_data.epoch_size,\n",
        "    epochs=10, \n",
        "    validation_data=validation_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cvLPgqlK01k",
        "outputId": "dcefc801-7e5d-42aa-85f6-f5a41554b5dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "57/57 [==============================] - 25s 438ms/step - loss: 15.9733 - accuracy: 0.5391 - val_loss: 7.5646 - val_accuracy: 0.7521\n",
            "Epoch 2/10\n",
            "57/57 [==============================] - 25s 444ms/step - loss: 4.2284 - accuracy: 0.8573 - val_loss: 2.6598 - val_accuracy: 0.9331\n",
            "Epoch 3/10\n",
            "57/57 [==============================] - 25s 436ms/step - loss: 2.0836 - accuracy: 0.9513 - val_loss: 1.6782 - val_accuracy: 0.9651\n",
            "Epoch 4/10\n",
            "57/57 [==============================] - 25s 442ms/step - loss: 1.4464 - accuracy: 0.9694 - val_loss: 1.1899 - val_accuracy: 0.9748\n",
            "Epoch 5/10\n",
            "57/57 [==============================] - 25s 436ms/step - loss: 1.1178 - accuracy: 0.9777 - val_loss: 0.9718 - val_accuracy: 0.9827\n",
            "Epoch 6/10\n",
            "57/57 [==============================] - 25s 436ms/step - loss: 0.9147 - accuracy: 0.9836 - val_loss: 0.7915 - val_accuracy: 0.9875\n",
            "Epoch 7/10\n",
            "57/57 [==============================] - 25s 434ms/step - loss: 0.7484 - accuracy: 0.9883 - val_loss: 0.6495 - val_accuracy: 0.9901\n",
            "Epoch 8/10\n",
            "57/57 [==============================] - 25s 437ms/step - loss: 0.6337 - accuracy: 0.9905 - val_loss: 0.6165 - val_accuracy: 0.9900\n",
            "Epoch 9/10\n",
            "57/57 [==============================] - 25s 437ms/step - loss: 0.5651 - accuracy: 0.9901 - val_loss: 0.5333 - val_accuracy: 0.9900\n",
            "Epoch 10/10\n",
            "57/57 [==============================] - 25s 441ms/step - loss: 0.5236 - accuracy: 0.9905 - val_loss: 0.4852 - val_accuracy: 0.9907\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2dc6db1a10>"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights('/content/test_weights.h5')"
      ],
      "metadata": {
        "id": "sS6i5pcGYVmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict"
      ],
      "metadata": {
        "id": "sv0ZyB9aYdRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_pred = build_model(num_classes=num_classes)\n",
        "model_pred.load_weights('/content/test_weights.h5')"
      ],
      "metadata": {
        "id": "BgSP2VWVYjfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "test_image_path = ['/content/contract_data/gz_image000_000_010.png', '/content/contract_data/gz_image000_000_011.png',\n",
        "                   '/content/contract_data/gz_image000_000_012.png', '/content/contract_data/gz_image000_000_013.png']\n",
        "image_list = []\n",
        "for path in test_image_path:\n",
        "    test_image = cv2.imread(path)\n",
        "    test_image = cv2.cvtColor(test_image, cv2.COLOR_BGR2GRAY)\n",
        "    test_image = cv2.resize(test_image, (280, 32))\n",
        "    test_image = test_image / 255. * 2.0 - 1.0\n",
        "    test_image = np.expand_dims(test_image, axis=-1)\n",
        "    image_list.append(test_image)\n"
      ],
      "metadata": {
        "id": "sQbmb9XgYezm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_list_images(image_list):\n",
        "    image_list = np.array(image_list)\n",
        "    # test_image = np.expand_dims(test_image, axis=0)\n",
        "    prob_matrix = model_pred.predict(image_list)\n",
        "    y_pred_len = np.full((len(image_list), ), int(image_list[0].shape[1] // 4))\n",
        "\n",
        "    y_pred_list, _ = keras.backend.ctc_decode(prob_matrix, y_pred_len, greedy=True)\n",
        "    pred_label_tensor = y_pred_list[0]\n",
        "    predictions = keras.backend.get_value(pred_label_tensor)\n",
        "    for single in predictions:\n",
        "        char = []\n",
        "        for label in single:\n",
        "            if label == -1:\n",
        "                continue\n",
        "            char.append(label_dict[label])\n",
        "\n",
        "        print(''.join(char))\n"
      ],
      "metadata": {
        "id": "Z_EbjCsnYnRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_list_images(image_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgoTH7DYaRNv",
        "outputId": "a66c171a-a853-4929-c902-f4f7b2a106af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "人民币¥11,442,186.\n",
            "14（大写\n",
            "人民币壹仟壹佰肆\n",
            "拾肆万贰仟壹佰捌\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_single_image(path):\n",
        "    test_image = cv2.imread(path)\n",
        "    test_image = cv2.cvtColor(test_image, cv2.COLOR_BGR2GRAY)\n",
        "    test_image = cv2.resize(test_image, (280, 32))\n",
        "    test_image = test_image / 255. * 2.0 - 1.0\n",
        "    test_image = np.expand_dims(test_image, axis=-1)\n",
        "    test_image = np.expand_dims(test_image, axis=0)\n",
        "\n",
        "    prob_matrix = model_pred.predict(test_image)\n",
        "    y_pred_len = np.full((len(test_image), ), int(test_image[0].shape[1] // 4))\n",
        "\n",
        "    y_pred_list, _ = keras.backend.ctc_decode(prob_matrix, y_pred_len, greedy=True)\n",
        "    pred_label_tensor = y_pred_list[0]\n",
        "    predictions = keras.backend.get_value(pred_label_tensor)\n",
        "\n",
        "    char = []\n",
        "    for label in predictions[0]:\n",
        "        if label == -1:\n",
        "            continue\n",
        "        char.append(label_dict[label])\n",
        "\n",
        "    print(''.join(char))\n",
        "predict_single_image(test_image_path[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfv6GqEOIEAo",
        "outputId": "71b62cbc-0d7f-4336-b930-e139c7e23636"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "人民币¥11,442,186.\n"
          ]
        }
      ]
    }
  ]
}